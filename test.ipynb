{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdfb6a94-7ffb-473a-88a3-67514133c2c8",
   "metadata": {},
   "source": [
    "### SEMICONDUCTOR MANUFACTURING PRODUCT QUALITY PREDICTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dc15ee-9b45-4847-8f61-cbce5b6d3e91",
   "metadata": {},
   "source": [
    "**Highlights:**\n",
    " <br> 1. Unlike with machine learning algorithms, \n",
    " <br> 1.A: no dataset outliers treatment as the auto-encoder neural netowrk is insensitive to them \n",
    " <br> 1.B: the multi-collinearity among the descriptors is also not bothered as the rigorous dimentionality reduction happens (quicker compared to PCA)\n",
    " <br> 2. Both the keras types of models are illustrated: Sequential API and Functional API\n",
    " <br> 3. All of the data is gathered from the sensors in real-time \n",
    " <br> 4. Dataset is labelled as 'Product Quality' where 1: Good 0: Bad\n",
    " <br> 5. Auto-encoder neural network is used as a dimentionality reduction technique\n",
    " <br> 6. Support vector machine uses the encoded data for the prediction of product quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1672dc32-fb2f-43aa-83a5-b3ef0d922852",
   "metadata": {},
   "source": [
    "Import all the necessary library packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "343db866-0eaf-4245-a565-656052090957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9366372b-25e1-4943-aedc-ced33e156533",
   "metadata": {},
   "source": [
    "Functions used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "492c3d86-1794-4c0e-94e2-a3538f422031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRedundantColumns(X):                                         \n",
    "    RedundantColumns = set()\n",
    "    for loc in range(X.shape[1]):\n",
    "        tocomparecolumn = X.iloc[:, loc]\n",
    "        for nextloc in range(loc + 1, X.shape[1]):\n",
    "            comparewithcolumn = X.iloc[:,nextloc]\n",
    "            if tocomparecolumn.equals(comparewithcolumn):\n",
    "                RedundantColumns.add(X.columns.values[nextloc])\n",
    "    return list(RedundantColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2758fbb-5a94-4c35-8eca-66e305e6a206",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "5d3fbc54-18a2-4b27-8081-1f35d85259de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time          0\n",
       "0             6\n",
       "1             7\n",
       "2            14\n",
       "3            14\n",
       "             ..\n",
       "586           1\n",
       "587           1\n",
       "588           1\n",
       "589           1\n",
       "Pass/Fail     0\n",
       "Length: 592, dtype: int64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('SMPQ.csv') \n",
    "dataset.isnull().sum()   # identifies the missing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0614f1d0-88c1-44fc-a7fc-b129d056d0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VPara-63417\\AppData\\Local\\Temp\\ipykernel_14800\\1731210136.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  dataset = dataset.fillna(dataset.median())   # fill the NaN values in each column with their respective column median\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Dataset dimension: (1567, 592)\n"
     ]
    }
   ],
   "source": [
    "dataset.replace('', np.nan, inplace=True)    # replace miising values across the dataset with NaN\n",
    "dataset = dataset.fillna(dataset.median())   # fill the NaN values in each column with their respective column median\n",
    "# dataset.isnull().sum() # to know the number of column missing values afterwards\n",
    "print('Actual Dataset dimension:',dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "9ac7ad59-2fee-4cbc-a8b5-4138884d79a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['515', '237', '258', '380', '402', '374', '231', '313', '178',\n",
       "       '508', '481', '191', '371', '414', '535', '514', '462', '534',\n",
       "       '266', '229', '403', '533', '243', '502', '260', '373', '265',\n",
       "       '97', '512', '397', '52', '236', '507', '240', '501', '505', '513',\n",
       "       '263', '400', '186', '458', '329', '69', '325', '233', '315',\n",
       "       '466', '451', '401', '179', '261', '230', '256', '504', '327',\n",
       "       '372', '422', '190', '330', '226', '276', '503', '531', '328',\n",
       "       '189', '192', '379', '404', '449', '529', '399', '464', '509',\n",
       "       '194', '375', '369', '242', '463', '530', '536', '532', '370',\n",
       "       '264', '141', '241', '394', '259', '322', '398', '193', '381',\n",
       "       '450', '234', '528', '262', '537', '149', '395', '364', '465',\n",
       "       '284', '314', '326', '257', '235', '498', '506', '232', '378',\n",
       "       '538', '396', '461'], dtype='<U3')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RedundantColumns = np.array(getRedundantColumns(dataset))   # Identifying the duplicate columns\n",
    "RedundantColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "bc07e795-cbcc-473c-a692-b5bac02cfa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimension after removing duplicate columns: (1567, 480)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.T.drop_duplicates().T    # removes all of the above duplicate columns \n",
    "print('Dataset dimension after removing duplicate columns:',dataset.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5206c71e-8e33-4f43-b135-015d17f98f62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimension after removing useless columns: (1567, 475)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.drop(['Time'], axis=1)  # Drop the 'Time' column as it is not needed\n",
    "\n",
    "dataconsistency = dataset.nunique()       # Identify whether the column data is identical for all the rows\n",
    "inconsistant_columns = dataconsistency[dataconsistency == 1].index\n",
    "dataset = dataset.drop(inconsistant_columns, axis=1) # drop the columns with no data variation\n",
    "print('Dataset dimension after removing useless columns:',dataset.shape)   # columns with all the rows having same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "3feb8d46-e5b5-4371-b5c8-115828c13680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate the class variable with the descriptors, as the normalization must be performed only with non-class variable\n",
    "Descriptors = dataset.iloc[:,:-1].values      \n",
    "Class = pd.DataFrame(dataset.iloc[:,-1].values)\n",
    "Class = Class.rename(columns={Class.columns[0]: 'Product Quality'})  # Renaming the label\n",
    "Class['Product Quality'] = Class['Product Quality'].replace([-1,1],[1,0])    # 1: Good, 0: Bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "f47cda07-dc5d-4d8a-946d-3f266b855f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "Descriptors = np.asarray(Descriptors).astype(np.float32)  # Performs the normalization\n",
    "layer = tf.keras.layers.Normalization(axis=None)\n",
    "layer.adapt(Descriptors)\n",
    "Normalised_Data=layer(Descriptors)\n",
    "Descriptors = pd.DataFrame(Normalised_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "dc1dc02e-f3b4-4b69-8353-ee7d92d2ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [Descriptors, Class]              # merge the class variable column with the normalized descriptors columns\n",
    "modified_dataset = pd.concat(dataframes, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c80bc9ff-61cd-4f53-9662-e78654cb667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_dataset.to_csv(r'SMPQ ModifiedDataset.csv')   # export the filtered dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12f2248-4449-4ccd-8591-aa65fded66eb",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4212b63-88d7-4a6d-9ca8-8b170b1552c0",
   "metadata": {},
   "source": [
    "Training the auto-encoder network (Funcitonal API Model), to use the bottleneck layer as the reduced dimentionality for prediction engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "76f46cd7-4a6e-4f2a-8e85-595f942fad10",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [233]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m input_layer \u001b[38;5;241m=\u001b[39m \u001b[43mInput\u001b[49m(shape \u001b[38;5;241m=\u001b[39m(Descriptors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], ))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "Guiding_layer = Input(shape =(Descriptors.shape[1], ))\n",
    "\n",
    "en_layer1 = Dense(300, activation ='tanh',activity_regularizer = regularizers.l1(0.01))(Guiding_layer)\n",
    "en_Layer2 = Dense(150, activation ='tanh',activity_regularizer = regularizers.l1(0.01))(en_layer1)\n",
    "en_Layer3 = Dense(75, activation ='tanh',activity_regularizer = regularizers.l1(0.01))(en_Layer2)\n",
    "en_Layer4 = Dense(32, activation ='tanh',activity_regularizer = regularizers.l1(0.01))(en_Layer3)\n",
    "\n",
    "Bottleneck_layer = Dense(15, activation ='relu')(en_Layer4)       # Compressed dimentionality\n",
    "  \n",
    "\n",
    "de_layer1 = Dense(32, activation ='tanh')(Bottleneck_layer)\n",
    "de_layer2 = Dense(75, activation ='tanh')(de_layer1)\n",
    "de_layer3 = Dense(150, activation ='tanh')(de_layer2)\n",
    "de_layer4 = Dense(300, activation ='tanh')(de_layer3)\n",
    "  \n",
    "reconstructed_layer = Dense(Descriptors.shape[1], activation ='relu')(de_layer4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e67e5-e29f-40c8-af57-f948a8d90ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
