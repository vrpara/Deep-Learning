{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef5cccd",
   "metadata": {},
   "source": [
    "### VIDEO SEGMENTATION BY CUSTOM ARCHITECTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d05840a",
   "metadata": {},
   "source": [
    "**Highlights:**\n",
    " <br> 1. Semantic Segmentation\n",
    " <br> 2. Image normalization \n",
    " <br> 3. Obtain Frames from Video and bundle back the segmented frames\n",
    " <br> 4. Early stopping and dropout to avoid model overfitting \n",
    " <br> 5. Works for live cameras\n",
    "\n",
    "Note: The code is just for conceptual implementations and did not utilise the actual organizational dataset. Hence, the training is very limited as the extensive training dataset is not available. Hence may not be precise for random footages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffca0e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import load_img,ImageDataGenerator,load_img, img_to_array,array_to_img\n",
    "from tensorflow.keras.layers import Conv2D, SeparableConv2D, Dropout,BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa5543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fetch_Data(dataset_path,dimension):\n",
    "    images = glob(os.path.join(dataset_path, \"original_images/*.png\"))\n",
    "    labels = glob(os.path.join(dataset_path, \"label_images/*.png\"))\n",
    "    image = []\n",
    "    label = []\n",
    "    for i in range (len(images)):\n",
    "        inp = plt.imread( images[i])\n",
    "        out = plt.imread( labels[i])        \n",
    "        inp = cv2.resize(inp, (dimension, dimension)) \n",
    "        out = cv2.resize(out, (dimension, dimension))\n",
    "        image.append(inp)\n",
    "        label.append(out)   \n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8597e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "image,label = Fetch_Data(\"Frame Segmentation\",256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e666b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SepConv2D(input, num_filters):\n",
    "    conv = SeparableConv2D(num_filters, 3, padding=\"same\")(input)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Activation(\"relu\")(conv)\n",
    "\n",
    "    conv = SeparableConv2D(num_filters, 3, padding=\"same\")(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Activation(\"relu\")(conv)\n",
    "\n",
    "    return conv\n",
    "\n",
    "def en(input, filters):\n",
    "    conv = SepConv2D(input, filters)\n",
    "    Agr = MaxPool2D((2, 2))(conv)\n",
    "    Agr = Dropout(0.05)(Agr)\n",
    "    return conv, Agr\n",
    "\n",
    "def de(input, skip_features, filters):\n",
    "    conv = Conv2DTranspose(filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "    conv = Concatenate()([conv, skip_features])\n",
    "    Agr = Dropout(0.07)(conv)\n",
    "    conv = SepConv2D(Agr, filters)\n",
    "    return conv\n",
    "\n",
    "def seg_deconv_model(input_shape,classes):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    cn1, Agr1 = en(inputs, 16)\n",
    "    cn2, Agr2 = en(Agr1, 32)\n",
    "    cn3, Agr3 = en(Agr2, 64)\n",
    "    cn4, Agr4 = en(Agr3, 128)\n",
    "\n",
    "    bottleneck = SepConv2D(Agr4, 256)\n",
    "\n",
    "    decn1 = de(bottleneck, cn4, 128)\n",
    "    decn2 = de(decn1, cn3, 64)\n",
    "    decn3 = de(decn2, cn2, 32)\n",
    "    decn4 = de(decn3, cn1, 16)\n",
    "\n",
    "    outputs = SeparableConv2D(classes, 1, padding=\"same\", activation=\"sigmoid\")(decn4)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abd54ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputsize = (256,256,3)\n",
    "neural_system = seg_deconv_model(inputsize, 3)\n",
    "\n",
    "neural_system.compile(loss=\"binary_crossentropy\",optimizer= 'Adam',metrics = ['accuracy'])\n",
    "callbacks = [\n",
    "        ModelCheckpoint(\"CustomArch.h5\", monitor=\"val_loss\", verbose=1),\n",
    "        ReduceLROnPlateau(monitor=\"val_loss\", patience=5, factor=0.1, verbose=1),\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea3caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_system.fit(np.array(image), np.array(label), epochs = 200, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b460e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "\n",
    "imagesFolder = \"images\"\n",
    "cap = cv2.VideoCapture(\"input_video_1.mp4\")\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# initialize the FourCC and a video writer object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output = cv2.VideoWriter('output_1.mp4', fourcc, 5, (frame_width, frame_height))\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    frameId = cap.get(1) #current frame number\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "      \n",
    "    if (ret != True):\n",
    "        break\n",
    "    for i in range(10):\n",
    "        filename = 'D:/images' + \"//\" + '%s.png' % i\n",
    "        cv2.imwrite(filename, frame)\n",
    "        \n",
    "    path = glob.glob('D:/images/*.png')\n",
    "    inp = []\n",
    "    for i in range (len(path)):\n",
    "        x = plt.imread( path[i])   \n",
    "        x = cv2.resize(x, (256, 256)) \n",
    "        inp.append(x)  \n",
    "  \n",
    "    inproc = np.array(inp)\n",
    "    labelled_frame = neural_system.predict(inproc)[1]\n",
    "    inp = []\n",
    "    min_val,max_val=labelled_frame.min(),labelled_frame.max()  \n",
    "    labelled_frame = 255.0*(labelled_frame - min_val)/(max_val - min_val)\n",
    "    labelled_frame = labelled_frame.astype(np.uint8)\n",
    "    labelled_frame = cv2.resize(labelled_frame, (frame_width, frame_height))\n",
    "#         filename = imagesFolder + \"/labelled_frame-\" +  str(int(frameId)) + \".png\"\n",
    "#         cv2.imwrite(filename, labelled_frame)\n",
    "    output.write(labelled_frame)\n",
    "\n",
    "cap.release()\n",
    "output.release()\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87cf8b9",
   "metadata": {},
   "source": [
    "### THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
