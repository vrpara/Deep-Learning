{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f368b102-36f0-43aa-b1bd-0b69bc7ddb72",
   "metadata": {},
   "source": [
    "### SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6e196a-3bb8-4c18-8c88-cded2df99893",
   "metadata": {},
   "source": [
    "**Highlights:**\n",
    " <br> 1. A binary classification model\n",
    " <br> 2. Object Oriented Approach \n",
    " <br> 3. AN end-point API that accepts english text and respond with the predicted sentiment \n",
    " <br> 4. Early stopping and dropout to avoid model overfitting \n",
    " \n",
    " Note: \n",
    " <br> A. The Epochs for the deep learning models can be further enhanced in the range from 30-50 for very precise accuracy\n",
    " <br> B. For API validation data when there is quotes within the string data it must be a single quotes with-in double quotes, for example: \"   'bad'  \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe4adea-adc9-4941-b5cd-e56f6a72f5ac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (1.1.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (0.24.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-learn) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-learn) (1.19.5)\n",
      "Requirement already satisfied: nltk in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (3.6.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from nltk) (2022.7.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from click->nltk) (4.8.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from importlib-metadata->click->nltk) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from importlib-metadata->click->nltk) (3.6.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tqdm->nltk) (5.4.0)\n",
      "Requirement already satisfied: fastapi in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (0.79.0)\n",
      "Requirement already satisfied: starlette==0.19.1 in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from fastapi) (0.19.1)\n",
      "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from fastapi) (1.9.1)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from starlette==0.19.1->fastapi) (4.1.1)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from starlette==0.19.1->fastapi) (3.6.1)\n",
      "Requirement already satisfied: contextlib2>=21.6.0 in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from starlette==0.19.1->fastapi) (21.6.0)\n",
      "Requirement already satisfied: contextvars in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from anyio<5,>=3.4.0->starlette==0.19.1->fastapi) (2.4)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from anyio<5,>=3.4.0->starlette==0.19.1->fastapi) (0.8)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from anyio<5,>=3.4.0->starlette==0.19.1->fastapi) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from anyio<5,>=3.4.0->starlette==0.19.1->fastapi) (3.3)\n",
      "Requirement already satisfied: immutables>=0.9 in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from contextvars->anyio<5,>=3.4.0->starlette==0.19.1->fastapi) (0.16)\n",
      "Requirement already satisfied: uvicorn in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (0.16.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from uvicorn) (4.1.1)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from uvicorn) (0.13.0)\n",
      "Requirement already satisfied: asgiref>=3.4.0 in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from uvicorn) (3.4.1)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from uvicorn) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from click>=7.0->uvicorn) (0.4.4)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from click>=7.0->uvicorn) (4.8.3)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from h11>=0.8->uvicorn) (0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\vpara\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from importlib-metadata->click>=7.0->uvicorn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install nltk\n",
    "!pip install fastapi\n",
    "!pip install uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52583f37-a39d-4b25-8687-bd8f7f4bf4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\చదువు మరియు సర్టిఫికేట్లు\\చ-22\\చ-22-MachineLearning-NeuralNets Projects\\7. Sentiment Analysis API\n"
     ]
    }
   ],
   "source": [
    "cd D:\\\\చదువు మరియు సర్టిఫికేట్లు\\\\చ-22\\\\చ-22-MachineLearning-NeuralNets Projects\\\\7. Sentiment Analysis API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74481310-2579-4ce4-a6bb-e05721f48a36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vpara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vpara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\vpara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vpara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nltk \n",
    "nltk.download('punkt') \n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from tensorflow.keras.layers import Flatten, LSTM, Dense, Embedding, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from fastapi import FastAPI\n",
    "import uvicorn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06cfd5b5-e6ab-4607-bbe0-d96c034793c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   airline_sentiment  \\\n",
       "0                  1   \n",
       "1                  0   \n",
       "2                  0   \n",
       "\n",
       "                                                                                                                             text  \n",
       "0                                                        @VirginAmerica plus you've added commercials to the experience... tacky.  \n",
       "1  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse  \n",
       "2                                                                         @VirginAmerica and it's a really big bad thing about it  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth',6000)\n",
    "data = pd.read_csv('airline_sentiment_analysis.csv',usecols = ['airline_sentiment','text'], low_memory = True)\n",
    "data.airline_sentiment.replace('positive', 1, inplace=True)\n",
    "data.airline_sentiment.replace('negative', 0, inplace=True)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "3153d173-e433-495b-b448-d75ca172183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.info()   # 11541 entries, 2 columns\n",
    "# data.isna().sum() # no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2cf10c1f-9a18-48a2-a564-2fa46733c5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentimentanalysis:\n",
    "    def __init__(self,data):\n",
    "        self.data = data        \n",
    "        \n",
    "    def textpreprocessing(self,inp):\n",
    "        \n",
    "        Pattern1 = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "        Pattern2 = '@[^\\s]+'\n",
    "        stopword = set(stopwords.words('english'))\n",
    "        inp = inp.lower()\n",
    "        \n",
    "        inp = re.sub(r\"yrs\", \"years\", inp)\n",
    "        inp = re.sub(r\"hrs\", \"hours\", inp)\n",
    "        inp = re.sub(r\"bday\", \"b-day\", inp)\n",
    "        inp = re.sub(r\"mother's\", \"mother\", inp)\n",
    "        inp = re.sub(r\"mom's\", \"mom\", inp)\n",
    "        inp = re.sub(r\"dad's\", \"dad\", inp)\n",
    "        inp = re.sub(r\"hahah|hahaha|hahahaha\", \"haha\", inp)\n",
    "        \n",
    "        inp = re.sub(r\"can't\", \"can not\", inp)\n",
    "        inp = re.sub(r\"wasn't\", \"was not\", inp)\n",
    "        inp = re.sub(r\"don't\", \"do not\", inp)\n",
    "        inp = re.sub(r\"aren't\", \"are not\", inp)\n",
    "        inp = re.sub(r\"isn't\", \"is not\", inp)\n",
    "        inp = re.sub(r\"won't\", \"will not\", inp)\n",
    "        inp = re.sub(r\"shouldn't\", \"should not\", inp)\n",
    "        inp = re.sub(r\"wouldn't\", \"would not\", inp)       \n",
    "        inp = re.sub(r\"haven't\", \"have not\", inp)\n",
    "        inp = re.sub(r\"hasn't\", \"has not\", inp)        \n",
    "        inp = re.sub(r\"couldn't\", \"could not\", inp)\n",
    "        inp = re.sub(r\"weren't\", \"were not\", inp)\n",
    "        inp = re.sub(r\"didn't\", \"did not\", inp)\n",
    "        inp = re.sub(r\"ain't\", \"am not\", inp)\n",
    "        inp = re.sub(r\"haven't\", \"have not\", inp)\n",
    "        inp = re.sub(r\"doesn't\", \"does not\", inp)\n",
    "\n",
    "        inp = re.sub(r\"he's\", \"he is\", inp)\n",
    "        inp = re.sub(r\"here's\", \"here is\", inp)\n",
    "        inp = re.sub(r\"what's\", \"what is\", inp)\n",
    "        inp = re.sub(r\"there's\", \"there is\", inp)\n",
    "        inp = re.sub(r\"he's\", \"he is\", inp)\n",
    "        inp = re.sub(r\"it's\", \"it is\", inp)\n",
    "        inp = re.sub(r\"there's\", \"there is\", inp)\n",
    "        inp = re.sub(r\"we're\", \"we are\", inp)\n",
    "        inp = re.sub(r\"that's\", \"that is\", inp)     \n",
    "        inp = re.sub(r\"who's\", \"who is\", inp)\n",
    "        inp = re.sub(r\"that's\", \"that is\", inp)\n",
    "        inp = re.sub(r\"where's\", \"where is\", inp)\n",
    "        inp = re.sub(r\"what's\", \"what is\", inp)\n",
    "        \n",
    "        inp = re.sub(r\"they're\", \"they are\", inp)\n",
    "        inp = re.sub(r\"you're\", \"you are\", inp)\n",
    "        inp = re.sub(r\"i'm\", \"I am\", inp)\n",
    "        inp = re.sub(r\"you've\", \"you have\", inp)\n",
    "        inp = re.sub(r\"we're\", \"we are\", inp)\n",
    "        inp = re.sub(r\"we've\", \"we have\", inp)\n",
    "        inp = re.sub(r\"y'all\", \"you all\", inp)\n",
    "        inp = re.sub(r\"would've\", \"would have\", inp)\n",
    "        inp = re.sub(r\"it'll\", \"it will\", inp)\n",
    "        inp = re.sub(r\"we'll\", \"we will\", inp)\n",
    "        inp = re.sub(r\"he'll\", \"he will\", inp)\n",
    "        inp = re.sub(r\"they'll\", \"they will\", inp)\n",
    "        inp = re.sub(r\"they'd\", \"they would\", inp) \n",
    "        inp = re.sub(r\"they've\", \"they have\", inp)\n",
    "        inp = re.sub(r\"i'd\", \"i would\", inp)\n",
    "        inp = re.sub(r\"should've\", \"should have\", inp)\n",
    "        inp = re.sub(r\"we'd\", \"we would\", inp)\n",
    "        inp = re.sub(r\"i'll\", \"I will\", inp)\n",
    "        inp = re.sub(r\"they're\", \"they are\", inp)\n",
    "        inp = re.sub(r\"let's\", \"let us\", inp)\n",
    "        inp = re.sub(r\"it's\", \"it is\", inp)\n",
    "        inp = re.sub(r\"you're\", \"you are\", inp)\n",
    "        inp = re.sub(r\"i've\", \"I have\", inp)\n",
    "        inp = re.sub(r\"you'll\", \"you will\", inp)\n",
    "        inp = re.sub(r\"you'd\", \"you would\", inp)\n",
    "        inp = re.sub(r\"could've\", \"could have\", inp)\n",
    "        inp = re.sub(r\"youve\", \"you have\", inp)  \n",
    "\n",
    "        inp = re.sub(Pattern1,'',inp)\n",
    "        inp = re.sub(Pattern2,'', inp) \n",
    "        inp = inp.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "        \n",
    "        \n",
    "        tokens = word_tokenize(inp)\n",
    "        my_tokens = [w for w in tokens if w not in stopword]\n",
    "        wordLemm = WordNetLemmatizer()\n",
    "        words=[]\n",
    "        for w in my_tokens:\n",
    "            if len(w)>1:\n",
    "                ele = wordLemm.lemmatize(w)\n",
    "                words.append(ele)\n",
    "\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def model_data(self):\n",
    "        self.data['text'] = self.data['text'].apply(lambda x: obj.textpreprocessing(x))\n",
    "        return self.data\n",
    "        \n",
    "        \n",
    "    def model_NB(self):            # Model 1 NAIVE BAYES\n",
    "        self.model_data()\n",
    "        self.count_vect =  CountVectorizer(max_features= 1000)\n",
    "        self.feature_vector = self.count_vect.fit(self.data.text)\n",
    "        self.data_features =  self.count_vect.transform(self.data.text)\n",
    "        self.train_x_m1, self.test_x_m1, self.train_y_m1, self.test_y_m1 =  train_test_split(self.data_features, self.data.airline_sentiment, \n",
    "                                                     test_size = 0.3, random_state = 42)\n",
    "        self.model_1 = MultinomialNB()\n",
    "        self.model_1.fit(self.train_x_m1.toarray(), self.train_y_m1)\n",
    "        self.predicted_model_1 = self.model_1.predict(self.test_x_m1.toarray())\n",
    "        \n",
    "        return self.count_vect, self.model_1\n",
    "  \n",
    "    def model_CNN(self):          # Model 2 Convolutional Neural Network\n",
    "        self.model_data()\n",
    "        self.text = self.data['text'].to_numpy()\n",
    "        self.sentiment = self.data['airline_sentiment'].to_numpy()\n",
    "        self.train_x_m2, self.test_x_m2, self.train_y_m2, self.test_y_m2  = train_test_split(self.text, self.sentiment, test_size=0.3,random_state = 42)\n",
    "        self.vocab_size = 10000\n",
    "        self.sequence_length = 1000\n",
    "        self.embedding_dim = 16\n",
    "        self.tokenizer = Tokenizer(num_words=self.vocab_size, oov_token=\"<OOV>\")\n",
    "        self.tokenizer.fit_on_texts(self.train_x_m2)\n",
    "        self.train_sequences = self.tokenizer.texts_to_sequences(self.train_x_m2)        \n",
    "        self.train_padded = pad_sequences(self.train_sequences, maxlen=self.sequence_length, padding='post', truncating='post')\n",
    "        self.test_sequences = self.tokenizer.texts_to_sequences(self.test_x_m2)\n",
    "        self.test_padded = pad_sequences(self.test_sequences, maxlen=self.sequence_length, padding='post', truncating='post')\n",
    "        \n",
    "        self.model_2 = Sequential()\n",
    "        self.model_2.add(Embedding(self.vocab_size, self.embedding_dim, input_length=self.sequence_length))\n",
    "        self.model_2.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "        self.model_2.add(MaxPooling1D(pool_size=2))\n",
    "        self.model_2.add(Flatten())\n",
    "        self.model_2.add(Dense(1, activation='sigmoid'))\n",
    "        self.model_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        self.callbacks = [EarlyStopping(patience=2)]\n",
    "        self.history_model_2 = self.model_2.fit(self.train_padded, self.train_y_m2, epochs=3, validation_data=(self.test_padded, self.test_y_m2), callbacks=self.callbacks)\n",
    "\n",
    "        return self.tokenizer, self.model_2\n",
    "        \n",
    "    def model_LSTM(self):             # Model 3 Recurrent Neural Network- LSTM\n",
    "\n",
    "        lstm_out = 32\n",
    "        self.model_3 = Sequential()        \n",
    "        self.model_3.add(Embedding(self.vocab_size, self.embedding_dim, input_length=self.sequence_length))        \n",
    "        self.model_3.add(Bidirectional(LSTM(lstm_out)))\n",
    "        self.model_3.add(Dense(10, activation='relu'))\n",
    "        self.model_3.add(Dense(1, activation='sigmoid'))\n",
    "        self.model_3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        self.history_model_3 = self.model_3.fit(self.train_padded, self.train_y_m2, epochs=3, validation_data=(self.test_padded, self.test_y_m2), callbacks=self.callbacks)\n",
    "\n",
    "        return self.model_3\n",
    "\n",
    "    def __repr__(self):\n",
    "        # All three model metrics\n",
    "        self.model_NB()\n",
    "        self.model_CNN()\n",
    "        self.model_LSTM()\n",
    "\n",
    "        model1_metrics = metrics.classification_report(self.test_y_m1, self.predicted_model_1)\n",
    "        model2_metrics = pd.DataFrame(self.history_model_2.history)\n",
    "        model3_metrics = pd.DataFrame(self.history_model_3.history)\n",
    "        \n",
    "        Output = ['THE METRICS FOR NAIVE BAYES CLASSIFIER: ', model1_metrics,'THE METRICS FOR CNN CLASSIFIER: ', str(model2_metrics), 'THE METRICS FOR LSTM CLASSIFIER: ',repr(model3_metrics)]\n",
    "\n",
    "        return   '\\n\\n'.join(Output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8185847-9b39-4316-89d7-d06297492e65",
   "metadata": {},
   "source": [
    "#### The Model Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea851c53-7ab6-427c-a68f-54888d6728f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253/253 [==============================] - 20s 75ms/step - loss: 0.4055 - accuracy: 0.8217 - val_loss: 0.2846 - val_accuracy: 0.8885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "THE METRICS FOR NAIVE BAYES CLASSIFIER: \n",
       "\n",
       "              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.94      0.94      0.94      2771\n",
       "           1       0.75      0.74      0.74       692\n",
       "\n",
       "    accuracy                           0.90      3463\n",
       "   macro avg       0.84      0.84      0.84      3463\n",
       "weighted avg       0.90      0.90      0.90      3463\n",
       "\n",
       "\n",
       "THE METRICS FOR CNN CLASSIFIER: \n",
       "\n",
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  0.405491  0.821738  0.284574      0.888536"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = sentimentanalysis(data)\n",
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89e5cd6-546b-40e0-bad2-b497ecc94dd4",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2290c71c-8e55-4e83-9e8d-7591905050b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-388e9c1038c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0muvicorn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\uvicorn\\main.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(app, **kwargs)\u001b[0m\n\u001b[0;32m    450\u001b[0m         \u001b[0mMultiprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msockets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m         \u001b[0mserver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pragma: py-win32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\uvicorn\\server.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, sockets)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msockets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msockets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_event_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msockets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msockets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;33masync\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mserve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msockets\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    473\u001b[0m         \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_done_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_run_until_complete_cb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_forever\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnew_task\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcancelled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\u001b[0m in \u001b[0;36mrun_forever\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'This event loop is already running'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             raise RuntimeError(\n",
      "\u001b[1;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [18488]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:53156 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:53157 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53157 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
      "253/253 [==============================] - 20s 76ms/step - loss: 0.3980 - accuracy: 0.8298 - val_loss: 0.2665 - val_accuracy: 0.8952\n",
      "253/253 [==============================] - 142s 540ms/step - loss: 0.4513 - accuracy: 0.8065 - val_loss: 0.3050 - val_accuracy: 0.8776\n"
     ]
    }
   ],
   "source": [
    "obj = sentimentanalysis(data)\n",
    "app = FastAPI(debug=True)\n",
    "@app.get(\"/Sentiment Analysis\")\n",
    "\n",
    "def predict(Validation_Data: str):\n",
    "    processed_data = obj.textpreprocessing(Validation_Data)\n",
    "    \n",
    "    x,y = obj.model_NB()\n",
    "    trans_data_1 = x.transform([processed_data])    \n",
    "    model_prediction_1 = y.predict(trans_data_1.toarray())\n",
    "    \n",
    "    df = pd.DataFrame({\"input_data\":[processed_data]})\n",
    "    trans_data_2_3 = df[\"input_data\"].to_numpy()\n",
    "    \n",
    "    m,n = obj.model_CNN()  \n",
    "    trans_data_2_3_seq = m.texts_to_sequences(trans_data_2_3)\n",
    "    trans_data_2_3_padded = pad_sequences(trans_data_2_3_seq, maxlen=1000, padding='post', truncating='post')\n",
    "    model_predict_2 = n.predict(trans_data_2_3_padded)\n",
    "    model_prediction_2 = 1 if model_predict_2[0][0] >= 0.70 else 0   \n",
    "\n",
    "    a = obj.model_LSTM()\n",
    "    model_predict_3 = a.predict(trans_data_2_3_padded)\n",
    "    model_prediction_3 = 1 if model_predict_3[0][0] >= 0.70 else 0   \n",
    "    \n",
    "    output = [model_prediction_1,model_prediction_2,model_prediction_3]    \n",
    "    output = [\"Positive\" if ele==1 else \"Negative\" for ele in output]\n",
    "            \n",
    "    api_out = {'The Review is:      '\n",
    "               + '{}'.format(output[0]) + ' '+'as per Naive Bayes Classifier' + '  -->          ' \n",
    "               + '{}'.format(output[1]) + ' '+ 'as per CNN Classifier' + '  -->          ' \n",
    "               + '{}'.format(output[2]) + ' '+ 'as per LSTM Classifier' }\n",
    "    \n",
    "    return api_out\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    uvicorn.run(app)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88548cfe-5e33-4b0d-bfab-8cd905bf1cad",
   "metadata": {},
   "source": [
    "# THE END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7806d0f2-7df0-4656-9efa-a76d520df43f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
