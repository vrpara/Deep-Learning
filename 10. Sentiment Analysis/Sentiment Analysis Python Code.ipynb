{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f368b102-36f0-43aa-b1bd-0b69bc7ddb72",
   "metadata": {},
   "source": [
    "### SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6e196a-3bb8-4c18-8c88-cded2df99893",
   "metadata": {},
   "source": [
    "**Highlights:**\n",
    " <br> 1. A binary classification model\n",
    " <br> 2. Object Oriented Approach \n",
    " <br> 3. AN end-point API that accepts english text and respond with the predicted sentiment \n",
    " <br> 4. Early stopping and dropout to avoid model overfitting \n",
    " \n",
    " Note: \n",
    " <br> A. The Epochs for the deep learning models can be further enhanced in the range from 30-50 for very precise accuracy\n",
    " <br> B. For API validation data when there is quotes within the string data it must be a single quotes with-in double quotes, for example: \"   'bad'  \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247d324c-b4d5-491c-a362-221dbd833644",
   "metadata": {},
   "source": [
    "#### 1. Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0fb539-21ee-4a84-9cd6-f65f8f441df5",
   "metadata": {},
   "source": [
    "##### 1.a Install the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe4adea-adc9-4941-b5cd-e56f6a72f5ac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\vpara\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vpara\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\vpara\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\vpara\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vpara\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\vpara\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\vpara\\anaconda3\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\vpara\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\vpara\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\vpara\\anaconda3\\lib\\site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: nltk in c:\\users\\vpara\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\vpara\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\vpara\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: click in c:\\users\\vpara\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vpara\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vpara\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Requirement already satisfied: fastapi in c:\\users\\vpara\\anaconda3\\lib\\site-packages (0.79.0)\n",
      "Requirement already satisfied: starlette==0.19.1 in c:\\users\\vpara\\anaconda3\\lib\\site-packages (from fastapi) (0.19.1)\n",
      "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in c:\\users\\vpara\\anaconda3\\lib\\site-packages (from fastapi) (1.9.1)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\vpara\\anaconda3\\lib\\site-packages (from starlette==0.19.1->fastapi) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in c:\\users\\vpara\\anaconda3\\lib\\site-packages (from starlette==0.19.1->fastapi) (4.1.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\vpara\\anaconda3\\lib\\site-packages (from anyio<5,>=3.4.0->starlette==0.19.1->fastapi) (3.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\vpara\\anaconda3\\lib\\site-packages (from anyio<5,>=3.4.0->starlette==0.19.1->fastapi) (1.2.0)\n",
      "Requirement already satisfied: uvicorn in c:\\users\\vpara\\anaconda3\\lib\\site-packages (0.18.2)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\vpara\\anaconda3\\lib\\site-packages (from uvicorn) (8.0.4)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\vpara\\anaconda3\\lib\\site-packages (from uvicorn) (0.13.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vpara\\anaconda3\\lib\\site-packages (from click>=7.0->uvicorn) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install nltk\n",
    "!pip install fastapi\n",
    "!pip install uvicorn\n",
    "pip install deta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52583f37-a39d-4b25-8687-bd8f7f4bf4cc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\చదువు మరియు సర్టిఫికేట్లు\\చ-22\\చ-22-MachineLearning-NeuralNets Projects\\7. Sentiment Analysis API\n"
     ]
    }
   ],
   "source": [
    "cd D:\\\\చదువు మరియు సర్టిఫికేట్లు\\\\చ-22\\\\చ-22-MachineLearning-NeuralNets Projects\\\\7. Sentiment Analysis API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700f18fe-7fd6-4c92-a02c-02cca0a5554f",
   "metadata": {},
   "source": [
    "##### 1.b Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74481310-2579-4ce4-a6bb-e05721f48a36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vpara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vpara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\vpara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vpara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# The general library packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# The necessary library packages for text pre-processing\n",
    "\n",
    "import nltk \n",
    "nltk.download('punkt') \n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "\n",
    "# The required library packages for Naive Bayes Classification\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# The required library packages for deep learning models\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from tensorflow.keras.layers import Flatten, LSTM, Dense, Embedding, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# The required library packages for end-point API\n",
    "import nest_asyncio\n",
    "from fastapi import FastAPI\n",
    "import uvicorn \n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f2e56f-9953-4060-a87e-5ddd5c1f457a",
   "metadata": {},
   "source": [
    "##### 1.c Fetch the Data set and do required modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06cfd5b5-e6ab-4607-bbe0-d96c034793c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   airline_sentiment  \\\n",
       "0                  1   \n",
       "1                  0   \n",
       "2                  0   \n",
       "\n",
       "                                                                                                                             text  \n",
       "0                                                        @VirginAmerica plus you've added commercials to the experience... tacky.  \n",
       "1  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse  \n",
       "2                                                                         @VirginAmerica and it's a really big bad thing about it  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth',6000)\n",
    "data = pd.read_csv('airline_sentiment_analysis.csv',usecols = ['airline_sentiment','text'], low_memory = True)\n",
    "data.airline_sentiment.replace('positive', 1, inplace=True)           # the label as positive is replaced with 1\n",
    "data.airline_sentiment.replace('negative', 0, inplace=True)           # the label as negative is replaced with 0\n",
    "data.head(3)                                                          # displays the dataframe content, first 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "3153d173-e433-495b-b448-d75ca172183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.info()   # 11541 entries, 2 columns\n",
    "# data.isna().sum() # no missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670a39cd-1455-405b-9aaf-927e083b5b23",
   "metadata": {},
   "source": [
    "#### 2. Sentiment Analysis Object-Oriented Design  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ddf6e6-ad66-4c58-88c9-087287295944",
   "metadata": {},
   "source": [
    "##### The following are the methods declared using the OOPS concept:\n",
    "<br> I. Text pre-processing: Filters the specific text , that includes proper wording, removing un-necessary symbols-punctuations-words and implementing lemmatization\n",
    "<br> II. Model Data: Text pre-processing of the entire 'text' column of the original dataset.\n",
    "<br> III. The Naive Bayes Classifier: Binary classification using the machine learning model as per Naive Bayes Theorm.\n",
    "<br> IV. The CNN Classifier: A convolutional deep learning model using tensorflow and keras.\n",
    "<br> V. The LSTM Classifier: A recurrent deep learning model using tensorflow and keras.\n",
    "<br> VI. The default representation method to display the model metrics.\n",
    "\n",
    "\n",
    "For all of the three models the dataset 'airline_sentiment' column is pre-processed and is set to match the model input criteria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cf10c1f-9a18-48a2-a564-2fa46733c5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentimentanalysis:\n",
    "    def __init__(self,data):\n",
    "        self.data = data        \n",
    "        \n",
    "    # Method for preprocessing the dataset review text\n",
    "    \n",
    "    def textpreprocessing(self,inp):                                 \n",
    "        \n",
    "        Pattern1 = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\" # pattern for filtering\n",
    "        Pattern2 = '@[^\\s]+'                                         # pattern for filtering\n",
    "        stopword = set(stopwords.words('english'))                   # set the words to remove from review content\n",
    "        inp = inp.lower()                                            # converts everything to lowercase\n",
    "        \n",
    "        # replace the shorcuts with proper user defined english words\n",
    "        inp = re.sub(r\"yrs\", \"years\", inp)\n",
    "        inp = re.sub(r\"hrs\", \"hours\", inp)\n",
    "        inp = re.sub(r\"bday\", \"b-day\", inp)\n",
    "        inp = re.sub(r\"mother's\", \"mother\", inp)\n",
    "        inp = re.sub(r\"mom's\", \"mom\", inp)\n",
    "        inp = re.sub(r\"dad's\", \"dad\", inp)\n",
    "        inp = re.sub(r\"hahah|hahaha|hahahaha\", \"haha\", inp)\n",
    "        \n",
    "        inp = re.sub(r\"can't\", \"can not\", inp)\n",
    "        inp = re.sub(r\"wasn't\", \"was not\", inp)\n",
    "        inp = re.sub(r\"don't\", \"do not\", inp)\n",
    "        inp = re.sub(r\"aren't\", \"are not\", inp)\n",
    "        inp = re.sub(r\"isn't\", \"is not\", inp)\n",
    "        inp = re.sub(r\"won't\", \"will not\", inp)\n",
    "        inp = re.sub(r\"shouldn't\", \"should not\", inp)\n",
    "        inp = re.sub(r\"wouldn't\", \"would not\", inp)       \n",
    "        inp = re.sub(r\"haven't\", \"have not\", inp)\n",
    "        inp = re.sub(r\"hasn't\", \"has not\", inp)        \n",
    "        inp = re.sub(r\"couldn't\", \"could not\", inp)\n",
    "        inp = re.sub(r\"weren't\", \"were not\", inp)\n",
    "        inp = re.sub(r\"didn't\", \"did not\", inp)\n",
    "        inp = re.sub(r\"ain't\", \"am not\", inp)\n",
    "        inp = re.sub(r\"haven't\", \"have not\", inp)\n",
    "        inp = re.sub(r\"doesn't\", \"does not\", inp)\n",
    "\n",
    "        inp = re.sub(r\"he's\", \"he is\", inp)\n",
    "        inp = re.sub(r\"here's\", \"here is\", inp)\n",
    "        inp = re.sub(r\"what's\", \"what is\", inp)\n",
    "        inp = re.sub(r\"there's\", \"there is\", inp)\n",
    "        inp = re.sub(r\"he's\", \"he is\", inp)\n",
    "        inp = re.sub(r\"it's\", \"it is\", inp)\n",
    "        inp = re.sub(r\"there's\", \"there is\", inp)\n",
    "        inp = re.sub(r\"we're\", \"we are\", inp)\n",
    "        inp = re.sub(r\"that's\", \"that is\", inp)     \n",
    "        inp = re.sub(r\"who's\", \"who is\", inp)\n",
    "        inp = re.sub(r\"that's\", \"that is\", inp)\n",
    "        inp = re.sub(r\"where's\", \"where is\", inp)\n",
    "        inp = re.sub(r\"what's\", \"what is\", inp)\n",
    "        \n",
    "        inp = re.sub(r\"they're\", \"they are\", inp)\n",
    "        inp = re.sub(r\"you're\", \"you are\", inp)\n",
    "        inp = re.sub(r\"i'm\", \"I am\", inp)\n",
    "        inp = re.sub(r\"you've\", \"you have\", inp)\n",
    "        inp = re.sub(r\"we're\", \"we are\", inp)\n",
    "        inp = re.sub(r\"we've\", \"we have\", inp)\n",
    "        inp = re.sub(r\"y'all\", \"you all\", inp)\n",
    "        inp = re.sub(r\"would've\", \"would have\", inp)\n",
    "        inp = re.sub(r\"it'll\", \"it will\", inp)\n",
    "        inp = re.sub(r\"we'll\", \"we will\", inp)\n",
    "        inp = re.sub(r\"he'll\", \"he will\", inp)\n",
    "        inp = re.sub(r\"they'll\", \"they will\", inp)\n",
    "        inp = re.sub(r\"they'd\", \"they would\", inp) \n",
    "        inp = re.sub(r\"they've\", \"they have\", inp)\n",
    "        inp = re.sub(r\"i'd\", \"i would\", inp)\n",
    "        inp = re.sub(r\"should've\", \"should have\", inp)\n",
    "        inp = re.sub(r\"we'd\", \"we would\", inp)\n",
    "        inp = re.sub(r\"i'll\", \"I will\", inp)\n",
    "        inp = re.sub(r\"they're\", \"they are\", inp)\n",
    "        inp = re.sub(r\"let's\", \"let us\", inp)\n",
    "        inp = re.sub(r\"it's\", \"it is\", inp)\n",
    "        inp = re.sub(r\"you're\", \"you are\", inp)\n",
    "        inp = re.sub(r\"i've\", \"I have\", inp)\n",
    "        inp = re.sub(r\"you'll\", \"you will\", inp)\n",
    "        inp = re.sub(r\"you'd\", \"you would\", inp)\n",
    "        inp = re.sub(r\"could've\", \"could have\", inp)\n",
    "        inp = re.sub(r\"youve\", \"you have\", inp)  \n",
    "\n",
    "        inp = re.sub(Pattern1,'',inp)\n",
    "        inp = re.sub(Pattern2,'', inp) \n",
    "        inp = inp.translate(str.maketrans(\"\",\"\",string.punctuation)) # removes punctuations form the review text\n",
    "        \n",
    "        \n",
    "        tokens = word_tokenize(inp)                                 # review text words tokenization\n",
    "        my_tokens = [w for w in tokens if w not in stopword]\n",
    "        wordLemm = WordNetLemmatizer()                              # Lemmatization, the morphological analysis of the review text\n",
    "        words=[]\n",
    "        for w in my_tokens:\n",
    "            if len(w)>1:\n",
    "                ele = wordLemm.lemmatize(w)\n",
    "                words.append(ele)\n",
    "\n",
    "        return ' '.join(words)                                     # the review text after pre-processing\n",
    "    \n",
    "    def model_data(self):                                          # the method that filters the entire dataset review text\n",
    "        self.data['text'] = self.data['text'].apply(lambda x: obj.textpreprocessing(x))\n",
    "        return self.data\n",
    "        \n",
    "    # MODEL 1 NAIVE BAYES \n",
    "    \n",
    "    def model_NB(self):                                           \n",
    "        self.model_data()                                                         # filters the dataset text column\n",
    "        \n",
    "        self.count_vect =  CountVectorizer(max_features= 1000)                    # groups text column words as a vector\n",
    "        self.feature_vector = self.count_vect.fit(self.data.text)                 # fit the countvectorizer methods\n",
    "        self.data_features =  self.count_vect.transform(self.data.text)           # transforms the text column words to match the model input\n",
    "        # split the dataset into train data and test data\n",
    "        self.train_x_m1, self.test_x_m1, self.train_y_m1, self.test_y_m1 =  train_test_split(self.data_features, self.data.airline_sentiment,test_size = 0.3, random_state = 42)\n",
    "        self.model_1 = MultinomialNB()                                            # constructs a naive Bayes Model, used mutinomial for enhanced performance\n",
    "        self.model_1.fit(self.train_x_m1.toarray(), self.train_y_m1)              # fit the model\n",
    "        self.predicted_model_1 = self.model_1.predict(self.test_x_m1.toarray())   # predict the test data for understanding the model metrics \n",
    "        \n",
    "        return self.count_vect, self.model_1                                      # these will be used in the API for respective model prediction\n",
    "  \n",
    "    # MODEL 2 CONVOLUTIONAL NEURAL NETWORK \n",
    "    \n",
    "    def model_CNN(self):          \n",
    "        self.model_data()                                                        # filters the dataset text column\n",
    "        \n",
    "        self.text = self.data['text'].to_numpy()                                 # converts the text column to n-dimensional array\n",
    "        self.sentiment = self.data['airline_sentiment'].to_numpy()               # converts the label column to n-dimensional array\n",
    "        # split the dataset into train data and test data\n",
    "        self.train_x_m2, self.test_x_m2, self.train_y_m2, self.test_y_m2  = train_test_split(self.text, self.sentiment, test_size=0.3,random_state = 42)\n",
    "        \n",
    "        self.vocab_size = 10000                                                  # model parameters\n",
    "        self.sequence_length = 1000\n",
    "        self.embedding_dim = 16\n",
    "        \n",
    "        self.tokenizer = Tokenizer(num_words=self.vocab_size, oov_token=\"<OOV>\")                                                # tokenization\n",
    "        self.tokenizer.fit_on_texts(self.train_x_m2)                                                                            # fit with train data\n",
    "        self.train_sequences = self.tokenizer.texts_to_sequences(self.train_x_m2)                                               # train data - convert to sequence       \n",
    "        self.train_padded = pad_sequences(self.train_sequences, maxlen=self.sequence_length, padding='post', truncating='post') # pad the train data sequence\n",
    "        self.test_sequences = self.tokenizer.texts_to_sequences(self.test_x_m2)                                                 # test data - convert to sequence \n",
    "        self.test_padded = pad_sequences(self.test_sequences, maxlen=self.sequence_length, padding='post', truncating='post')   # pad the test data sequence\n",
    "        \n",
    "        self.model_2 = Sequential()                                                                         # Construct a convolutional model using keras sequential API\n",
    "        self.model_2.add(Embedding(self.vocab_size, self.embedding_dim, input_length=self.sequence_length)) # An embedded layer for input text\n",
    "        self.model_2.add(Conv1D(filters=128, kernel_size=5, activation='relu'))                             # convolutional layer\n",
    "        self.model_2.add(MaxPooling1D(pool_size=2))                                                         # max-pool layer\n",
    "        self.model_2.add(Flatten())                                                                         # flatten the output from max-pool layer\n",
    "        self.model_2.add(Dense(1, activation='sigmoid'))                                                    # activation function\n",
    "        self.model_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])            # compile the model\n",
    "        self.callbacks = [EarlyStopping(patience=2)]                                                        # define a model callback, in this case only early stopping is used\n",
    "        # fit the model, very less epoch is used (for quick API deomnstration), in practise it must be much more for a very precise classification\n",
    "        self.history_model_2 = self.model_2.fit(self.train_padded, self.train_y_m2, epochs=3, validation_data=(self.test_padded, self.test_y_m2), callbacks=self.callbacks)\n",
    "\n",
    "        return self.tokenizer, self.model_2                                                                 # these will be used in the API for respective model prediction\n",
    "    \n",
    "    # MODEL 3 LSTM RECURRENT NEURAL NETWORK  \n",
    "    \n",
    "    def model_LSTM(self):             \n",
    "\n",
    "        lstm_out = 32\n",
    "        self.model_3 = Sequential()                                                                         # Construct a recurrent model using keras sequential API\n",
    "        self.model_3.add(Embedding(self.vocab_size, self.embedding_dim, input_length=self.sequence_length)) # An embedded layer for input text        \n",
    "        self.model_3.add(Bidirectional(LSTM(lstm_out)))                                                     # Recurrent layer\n",
    "        self.model_3.add(Dense(10, activation='relu'))                                                      # Activation layer- first level\n",
    "        self.model_3.add(Dense(1, activation='sigmoid'))                                                    # Activation layer- output level\n",
    "        self.model_3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])            # compile the model\n",
    "        # fit the model, very less epoch is used (for quick API deomnstration), in practise it must be much more for a very precise classification\n",
    "        self.history_model_3 = self.model_3.fit(self.train_padded, self.train_y_m2, epochs=3, validation_data=(self.test_padded, self.test_y_m2), callbacks=self.callbacks)\n",
    "\n",
    "        return self.model_3                                                                                 # these will be used in the API for respective model prediction\n",
    "\n",
    "    # The method for populating the performance metrics of all the three models\n",
    "    def __repr__(self):\n",
    "        # All three model metrics\n",
    "        self.model_NB()                                                                                    # calls the naive bayes model method\n",
    "        self.model_CNN()                                                                                   # calls the CNN model method\n",
    "        self.model_LSTM()                                                                                  # calls the LSTM model method\n",
    "\n",
    "        model1_metrics = metrics.classification_report(self.test_y_m1, self.predicted_model_1)             # Pulls the naive bayes model performance metrics\n",
    "        model2_metrics = pd.DataFrame(self.history_model_2.history)                                        # Pulls the CNN model performance metrics\n",
    "        model3_metrics = pd.DataFrame(self.history_model_3.history)                                        # Pulls the LSTM performance metrics\n",
    "        \n",
    "        Output = ['THE METRICS FOR NAIVE BAYES CLASSIFIER: ', model1_metrics,'THE METRICS FOR CNN CLASSIFIER: ', str(model2_metrics), 'THE METRICS FOR LSTM CLASSIFIER: ',repr(model3_metrics)]\n",
    "\n",
    "        return   '\\n\\n'.join(Output)                                                                       # returns all of the model metrics as string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8185847-9b39-4316-89d7-d06297492e65",
   "metadata": {},
   "source": [
    "#### 3. The Model Metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e5dcdf-2d28-4e61-b19f-547f3da02aae",
   "metadata": {},
   "source": [
    "As a default step: Upon object declaration of the sentiment analysis class the model metrics will be displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea851c53-7ab6-427c-a68f-54888d6728f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "253/253 [==============================] - 21s 80ms/step - loss: 0.3972 - accuracy: 0.8245 - val_loss: 0.2730 - val_accuracy: 0.8920\n",
      "Epoch 2/3\n",
      "253/253 [==============================] - 19s 74ms/step - loss: 0.1877 - accuracy: 0.9289 - val_loss: 0.2273 - val_accuracy: 0.9096\n",
      "Epoch 3/3\n",
      "253/253 [==============================] - 19s 74ms/step - loss: 0.1024 - accuracy: 0.9635 - val_loss: 0.2467 - val_accuracy: 0.9062\n",
      "Epoch 1/3\n",
      "253/253 [==============================] - 125s 473ms/step - loss: 0.4031 - accuracy: 0.8373 - val_loss: 0.2307 - val_accuracy: 0.9070\n",
      "Epoch 2/3\n",
      "253/253 [==============================] - 144s 569ms/step - loss: 0.1686 - accuracy: 0.9344 - val_loss: 0.2068 - val_accuracy: 0.9209\n",
      "Epoch 3/3\n",
      "253/253 [==============================] - 134s 529ms/step - loss: 0.0963 - accuracy: 0.9661 - val_loss: 0.2443 - val_accuracy: 0.9128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "THE METRICS FOR NAIVE BAYES CLASSIFIER: \n",
       "\n",
       "              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.94      0.94      0.94      2771\n",
       "           1       0.74      0.75      0.75       692\n",
       "\n",
       "    accuracy                           0.90      3463\n",
       "   macro avg       0.84      0.84      0.84      3463\n",
       "weighted avg       0.90      0.90      0.90      3463\n",
       "\n",
       "\n",
       "THE METRICS FOR CNN CLASSIFIER: \n",
       "\n",
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  0.397175  0.824462  0.272987      0.892001\n",
       "1  0.187712  0.928943  0.227260      0.909616\n",
       "2  0.102438  0.963481  0.246673      0.906151\n",
       "\n",
       "THE METRICS FOR LSTM CLASSIFIER: \n",
       "\n",
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  0.403052  0.837336  0.230674      0.907017\n",
       "1  0.168591  0.934390  0.206804      0.920878\n",
       "2  0.096254  0.966081  0.244344      0.912792"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = sentimentanalysis(data)\n",
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89e5cd6-546b-40e0-bad2-b497ecc94dd4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 4. Sentiment Analysis API\n",
    "\n",
    "<br> I. An end-point API is constructed using FASTAPI\n",
    "<br> II. Accepts the data for validation as string \n",
    "<br> III. The three models respectively predicts the sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2290c71c-8e55-4e83-9e8d-7591905050b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [14104]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:57996 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:57997 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:57997 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
      "Epoch 1/3\n",
      "253/253 [==============================] - 22s 85ms/step - loss: 0.3950 - accuracy: 0.8293 - val_loss: 0.2798 - val_accuracy: 0.8903\n",
      "Epoch 2/3\n",
      "253/253 [==============================] - 21s 82ms/step - loss: 0.1924 - accuracy: 0.9249 - val_loss: 0.2352 - val_accuracy: 0.9024\n",
      "Epoch 3/3\n",
      "253/253 [==============================] - 21s 83ms/step - loss: 0.1041 - accuracy: 0.9629 - val_loss: 0.2617 - val_accuracy: 0.8932\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "Epoch 1/3\n",
      "253/253 [==============================] - 153s 593ms/step - loss: 0.4218 - accuracy: 0.8266 - val_loss: 0.2432 - val_accuracy: 0.9033\n",
      "Epoch 2/3\n",
      "253/253 [==============================] - 150s 595ms/step - loss: 0.1714 - accuracy: 0.9346 - val_loss: 0.2137 - val_accuracy: 0.9160\n",
      "Epoch 3/3\n",
      "253/253 [==============================] - 146s 577ms/step - loss: 0.1031 - accuracy: 0.9636 - val_loss: 0.2309 - val_accuracy: 0.9108\n",
      "1/1 [==============================] - 1s 915ms/step\n",
      "INFO:     127.0.0.1:58003 - \"GET /Sentiment%20Analysis?Validation_Data=%40united%20Late%20Flight%20to%20Denver%2C%20%40%21%22xyzr%20%22%20Late%20Flight%20to%20Newark...let%27s%20not%20even%20get%20into%20the%20disaster%20that%20was%20checking%20bags.%20Unacceptable.%40%21%28h HTTP/1.1\" 200 OK\n",
      "Epoch 1/3\n",
      "253/253 [==============================] - 25s 96ms/step - loss: 0.4048 - accuracy: 0.8241 - val_loss: 0.2802 - val_accuracy: 0.8854\n",
      "Epoch 2/3\n",
      "253/253 [==============================] - 25s 98ms/step - loss: 0.1954 - accuracy: 0.9237 - val_loss: 0.2250 - val_accuracy: 0.9096\n",
      "Epoch 3/3\n",
      "253/253 [==============================] - 20s 81ms/step - loss: 0.1080 - accuracy: 0.9619 - val_loss: 0.2436 - val_accuracy: 0.9134\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "Epoch 1/3\n",
      "253/253 [==============================] - 164s 636ms/step - loss: 0.4776 - accuracy: 0.7914 - val_loss: 0.3351 - val_accuracy: 0.8198\n",
      "Epoch 2/3\n",
      "253/253 [==============================] - 162s 641ms/step - loss: 0.2276 - accuracy: 0.9176 - val_loss: 0.2168 - val_accuracy: 0.9163\n",
      "Epoch 3/3\n",
      "253/253 [==============================] - 153s 604ms/step - loss: 0.1204 - accuracy: 0.9570 - val_loss: 0.2220 - val_accuracy: 0.9168\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "INFO:     127.0.0.1:58071 - \"GET /Sentiment%20Analysis?Validation_Data=%40virginair%20the%20flight%20was%20not%20in%20good%20condition.%20effected%20departure.%20delayed%20business%20meeting.%20bye.%40%22xzuigqb%21%40%22 HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()                                               # runs threads asyncronously\n",
    "\n",
    "obj = sentimentanalysis(data)                                      # object declaration\n",
    "\n",
    "app = FastAPI(debug=True)                                          # declares the API usage\n",
    "@app.get(\"/Sentiment Analysis\")\n",
    "\n",
    "def predict(Validation_Data):                                      # API predict function for the data to be validated\n",
    "    processed_data = obj.textpreprocessing(Validation_Data)        # text pre-processing\n",
    "    \n",
    "    x,y = obj.model_NB()                                           # Naive Bayes model prediction\n",
    "    trans_data_1 = x.transform([processed_data])    \n",
    "    model_prediction_1 = y.predict(trans_data_1.toarray())\n",
    "    \n",
    "    df = pd.DataFrame({\"input_data\":[processed_data]})             # Data preparation for next models \n",
    "    trans_data_2_3 = df[\"input_data\"].to_numpy()\n",
    "    \n",
    "    m,n = obj.model_CNN()                                          # CNN model prediction\n",
    "    trans_data_2_3_seq = m.texts_to_sequences(trans_data_2_3)\n",
    "    trans_data_2_3_padded = pad_sequences(trans_data_2_3_seq, maxlen=1000, padding='post', truncating='post')\n",
    "    model_predict_2 = n.predict(trans_data_2_3_padded)             # the output is a float value\n",
    "    model_prediction_2 = 1 if model_predict_2[0][0] >= 0.70 else 0 # binary value as per probability   \n",
    "\n",
    "    a = obj.model_LSTM()                                           # LSTM model prediction\n",
    "    model_predict_3 = a.predict(trans_data_2_3_padded)             # the output is a float value\n",
    "    model_prediction_3 = 1 if model_predict_3[0][0] >= 0.70 else 0 # binary value as per probability  \n",
    "    \n",
    "    output = [model_prediction_1,model_prediction_2,model_prediction_3] # A list of precited values   \n",
    "    output = [\"Positive\" if ele==1 else \"Negative\" for ele in output]   # translates to original sentiment category\n",
    "            \n",
    "    api_out = {'The Review is:      '\n",
    "               + '{}'.format(output[0]) + ' '+'as per Naive Bayes Classifier' + '  -->          ' \n",
    "               + '{}'.format(output[1]) + ' '+ 'as per CNN Classifier' + '  -->          ' \n",
    "               + '{}'.format(output[2]) + ' '+ 'as per LSTM Classifier' }\n",
    "    \n",
    "    return api_out\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    uvicorn.run(app)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fcc642-c2fd-4aad-b7bc-e05296f09d23",
   "metadata": {},
   "source": [
    "#### 5. Deploying API\n",
    "An end-point API can be deployed using deta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88548cfe-5e33-4b0d-bfab-8cd905bf1cad",
   "metadata": {},
   "source": [
    "# THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
